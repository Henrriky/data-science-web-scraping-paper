\section{Aplicações e Usos}

\subsection{Aplicações gerais e práticas}

Antes de entrar por áreas específicas, é válido trazer uma breve contextualização. Quando a raspagem de dados é combinada a modelos de IA, ela deixa de ser “coleta bruta” e passa a sustentar decisões do dia a dia — de acompanhar preços e notícias a apoiar diagnósticos e políticas públicas. Sempre que houver canais oficiais ou repositórios abertos (por exemplo, a coleção pública Common Crawl para textos da web, os catálogos acadêmicos Crossref e OpenAlex, ou o observatório de indicadores da Organização Mundial da Saúde), a coleta fica mais estável, reprodutível e auditável; quando esses canais não existem, a raspagem controlada ainda pode gerar valor, desde que venha acompanhada de limites de requisição, registro de versões e checagens de qualidade.

No varejo on-line, a coleta automatizada apoia o acompanhamento de preços, disponibilidade e tendências de consumo \cite{promptcloud_applications_2024}. Estudos econômicos que construíram índices a partir de milhões de preços coletados na web mostraram que esses índices andam de perto com os indicadores oficiais e ajudam a ler o mercado com mais rapidez. Revisões metodológicas em periódicos de estatística também discutem como transformar esses preços raspados em séries consistentes, comparando fórmulas e cuidados de qualidade.

No mercado financeiro, o uso de raspagem e IA aparece em dois eixos. Primeiro, na análise textual de documentos corporativos (como demonstrações e relatórios), que deu origem a dicionários e práticas de leitura automática hoje consolidadas em finanças. Segundo, no avanço dos “dados alternativos” (notícias, avaliações de produtos, sinais da web) para previsão, gestão de risco e monitoramento — tema que já conta com revisões amplas na literatura. Em paralelo, pesquisas recentes mostram que modelos modernos (incluindo redes em grafos) vêm ampliando o desempenho na detecção de fraude.

Na saúde, há duas frentes claras. Para suporte clínico com recuperação de evidências, surgiram levantamentos e protótipos que integram recuperação e geração (RAG) com bases biomédicas, discutindo benefícios e limites de precisão factual. Para vigilância epidemiológica, um marco foi o painel interativo da Universidade Johns Hopkins, descrito em revista médica de alto impacto, que integrou múltiplas fontes públicas para acompanhar a COVID-19 em tempo real — um modelo de organização de coleta, validação e abertura de dados em crises sanitárias. 

Na mídia e comunicação, bases que agregam notícias em múltiplos idiomas são usadas para mapear eventos, lugares e o “tom” de coberturas ao longo do tempo, com documentação acadêmica do seu processo de codificação e atualização. Para entender consumo e confiança em notícias, o Digital News Report 2025 (Universidade de Oxford) oferece séries comparáveis entre países; e, no combate à desinformação, há levantamentos de referência que revisam métodos de checagem automatizada, conjuntos de dados e desafios de interpretabilidade.

Em síntese, em todos os domínios, o valor do web scraping aliado à IA depende de três cuidados práticos: priorizar fontes e estudos reconhecidos (artigos, relatórios técnicos de pesquisa), padronizar qualidade ao longo do ETL (registros de coleta, amostragem, validações) e delimitar riscos e escopo de uso (privacidade, direitos, atualização). Esses elementos transformam a coleta em evidência útil para produtos analíticos, modelos e decisões, em linha com análises de mercado que tratam o web scraping como infraestrutura crítica de dados \cite{mordor_webscraping_market_2025}.

\subsection{Usos organizacionais por nível}

A aplicação do \textit{web scraping} combinado ao uso de IA pode ser compreendida de forma mais estruturada quando analisada sob a ótica dos níveis organizacionais — operacional, tático e estratégico. Cada um desses níveis demanda informações com diferentes graus de detalhamento e propósito analítico que, quando integrados corretamente, são essenciais para transformar dados em inteligência organizacional.

No nível operacional, o objetivo está na automação de tarefas repetitivas e na eficiência de processos. O \textit{web scraping} atua como ferramenta de coleta contínua e sistemática de dados que alimentam rotinas diárias, reduzindo a necessidade de trabalho manual \cite{promptcloud_applications_2024,crawlbase_web_ml_2025}. Entre as aplicações mais comuns estão o monitoramento de preços e estoques em \textit{e-commerces}, a coleta de avaliações e comentários de clientes em plataformas e a geração automática de relatórios básicos de desempenho. Quando associado a modelos de IA, esses dados podem ser classificados e resumidos em um intervalo de tempo menor, permitindo respostas rápidas e atualizadas com base nas solicitações.

No nível tático, a raspagem de dados tem finalidade mais analítica. Os dados extratos são normalmente integrados a sistemas de \textit{Business Intelligence} (BI) e transformados em \textit{dashboards} que mostram tendências de mercado, comportamento de clientes e desempenho da empresa. O uso de técnicas de aprendizado de máquina sobre os dados coletados permite identificar padrões e anomalias, enriquecendo relatórios gerenciais com \textit{insights} preditivos, em sintonia com análises de mercado que apontam o web scraping como componente central de soluções analíticas e de monitoramento competitivo \cite{mordor_webscraping_market_2025}.

No nível estratégico, a combinação de \textit{web scraping} e IA assume uma função de suporte à inovação e à formulação de políticas corporativas de longo prazo. As informações coletadas em larga escala podem alimentar modelos de previsão (\textit{forecasting}), análises de competitividade e mecanismos de detecção de tendências emergentes. Empresas de tecnologia, finanças e varejo, por exemplo, utilizam \textit{pipelines} de \textit{scraping} e modelos de linguagem para antecipar mudanças no comportamento do consumidor, movimentações regulatórias ou novas oportunidades de mercado \cite{grepsr_competitive_intel_2025}. Além disso, bases raspadas e tratadas adequadamente podem servir como insumo para projetos de inovação em produtos e serviços baseados em dados, fortalecendo a vantagem competitiva e a cultura \textit{data-driven} das organizações.

Em suma, os usos organizacionais do \textit{web scraping} aliado à IA se estendem por todos os níveis da hierarquia empresarial — da automação operacional à inteligência estratégica —, reforçando a importância dessa tecnologia como pilar da transformação digital e da competitividade baseada em dados.

\subsection{Aplicações práticas com modelos de linguagem de grande escala (LLMs)}

Com o advento da IA, principalmente dos modelos de linguagem de grande escala (LLMs), observou-se uma oportunidade dentro do mercado para utilizar essa ferramenta no apoio ao atendimento externo e interno das companhias, que tradicionalmente era realizado por funcionários com bases de conhecimento internas. No entanto, como esses modelos possuem conhecimento limitado aos dados de treinamento, sua aplicação prática tornou-se desafiadora, uma vez que dados privados das companhias não estariam disponíveis para que o modelo formulasse respostas adequadas. Para superar esses desafios, surgiram técnicas complementares que permitem adaptar os modelos aos contextos corporativos.

\subsubsection{Ajuste fino (\textit{Fine-tuning})}

A primeira técnica utilizada para adaptar as respostas de um modelo de LLM, é o “ajuste fino” ou “fine-tuning”, que consiste no processo de adaptação de um modelo treinado previamente para tarefas ou casos de uso específicos. Sua ideia principal é de aproveitar o conhecimento existente do modelo como ponto de partida para aprender novas tarefas, o que permite uma otimização de custo por não precisar treinar um modelo do zero. 

O ajuste fino é considerado uma técnica que utiliza-se do aprendizado supervisionado, uma vez que para realizar o treinamento de um modelo de LLM para um domínio específico, é necessário fornecer um conjunto de dados rotulados. É nessa etapa que o \textit{web scraping} pode atuar como uma ferramenta essencial para realizar a obtenção das informações, que eventualmente podem não estar disponíveis de forma oficial. Seu papel entra diretamente na capacidade de extrair dados rotulados a partir de páginas web que estejam alinhadas com o objetivo final do modelo que será treinado, transformando eles em dados rotulados que são utilizados dentro do processo de \textit{fine-tuning}. A título de exemplo, podemos citar uma uma página web de perguntas e respostas de uma empresa, que contém dados rotulados (as respostas) e podem ser facilmente transformados para realizar o aprendizado supervisionado por meio do \textit{fine-tuning}. Abaixo, temos um exemplo de conjunto de dados disponível na plataforma Hugging Face \cite{huggingface_dataset}, que pode ser utilizado para realizar o ajuste fino de modelos de linguagem.

\begin{figure}[ht]
  %%% TODO: Referência da Imagem %%%
  \centering
  \includegraphics[width=\linewidth]{cap04-aplicacoes-e-usos/assets/fine-tuning-image-1.pdf}
  \caption{Conjunto de dados do Hugging Face para fine-tuning de modelos de linguagem.}
  \label{fig:fine-tuning-image-1}
\end{figure}

Essa técnica costuma ser oferecida como serviço na nuvem, diretamente nos servidores das empresas que possuem um modelo de linguagem proprietário. A OpenAI, por exemplo, permite realizar o envio de arquivos CSV contendo a variável independente e a dependente (alvo). Após a execução do processo de \textit{fine-tuning} com o conjunto de dados enviado, gera-se um modelo com identificador único que pode ser utilizado posteriormente, seja por meio de chamadas de API, para integrar com aplicações empresariais, ou pela própria interface da OpenAI. Por outro lado, existem modelos de LLM que são disponibilizados de forma gratuita através de plataformas como o Hugging Face e que podem ser baixados em servidores dedicados, sendo possível aplicar o mesmo processo de fine-tuning que é utilizado dentro de plataformas de empresas privadas. Abaixo, temos um diagrama ilustrativo do processo de fine-tuning utilizando dados obtidos por meio do \textit{web scraping}, que foi elaborado pelos autores deste artigo:

\begin{figure}[ht]
  %%% TODO: Referência da Imagem %%%
  \centering
  \includegraphics[width=\linewidth]{cap04-aplicacoes-e-usos/assets/fine-tuning-image-2.pdf}
  \caption{Arquitetura típica para fine-tuning de modelos de linguagem com dados obtidos por Web Scraping.}
  \label{fig:fine-tuning-image-2}
\end{figure}


\subsubsection{RAG (Retrieval-Augmented Generation)}

O \textit{Retrieval-Augmented Generation} (RAG) é um padrão de arquitetura no qual um modelo de linguagem (LLM) gera respostas condicionadas a uma base de conhecimento externa, armazenada em um mecanismo que permite a busca semântica de conteúdos. Esse acoplamento entre recuperação e geração reduz o risco de alucinações, mantém a atualidade do conteúdo sem necessidade de treinamentos frequentes e possibilita a citação explícita das fontes utilizadas. Quando comparado ao \textit{fine-tuning}, o RAG mostra-se mais adequado em contextos em que o conhecimento da organização é dinâmico, sujeito a alterações frequentes, ou quando há exigências de auditoria e restrições de acesso a dados sensíveis que não devem ser incorporados permanentemente ao modelo. O \textit{fine-tuning}, por sua vez, mantém-se vantajoso em cenários nos quais se busca induzir estilo de escrita, formato de saída ou comportamentos específicos de tarefa, bem como generalizações a partir de exemplos rotulados. Na prática, ambas as abordagens são complementares: o RAG é mais apropriado para a gestão de conteúdo dinâmico e que possa conter citações, enquanto o \textit{fine-tuning} é indicado para a adequação comportamental e de formato.

O \textit{pipeline} típico do RAG inicia-se com a etapa de ingestão de conteúdo, na qual o \textit{web scraping} desempenha papel fundamental na coleta automatizada de páginas autorizadas, como FAQs, políticas corporativas, manuais e documentações técnicas. O material coletado passa por processos de limpeza e normalização — por exemplo, conversão de HTML para texto puro ou Markdown —, sendo posteriormente enriquecido com metadados relevantes, como URL, título, data e versão. Essa etapa visa reduzir ruídos e garantir a consistência das informações que serão utilizadas.

Em seguida, realiza-se a segmentação textual, dividindo o conteúdo em passagens curtas com sobreposição controlada e alinhamento aos títulos das páginas. Essa técnica preserva a coesão textual e otimiza a etapa de recuperação semântica. Após a segmentação, cada trecho é convertido em uma representação vetorial (*embedding*), ou seja, uma forma numérica de representar o significado semântico do conteúdo. Essa transformação é realizada por modelos especializados, como o \textit{text-embedding-3-large}, da OpenAI. Os vetores resultantes são armazenados em bancos de dados vetoriais — como Chroma, Pinecone, Weaviate e pgvector —, que permitem a recuperação semântica das informações com base na similaridade entre a consulta e os vetores armazenados. Entre as principais métricas utilizadas destacam-se a \textit{Mean Reciprocal Rank} (MRR) e a similaridade de cosseno.

A última etapa do \textit{pipeline} envolve a integração com o modelo de linguagem, na qual se implementa a lógica de consulta semântica e recuperação das informações a partir do banco de dados vetorial. Atualmente, muitos LLMs são treinados com suporte a chamadas de ferramentas (\textit{tool calling}), mecanismo que permite ao modelo, durante a inferência, acionar de forma programática uma função externa previamente definida. Essa função pode executar uma busca semântica e retornar seus resultados, que são adicionados ao histórico de mensagens da sessão, servindo como contexto adicional para o modelo gerar uma resposta ao usuário final.

Nesse contexto, o \textit{Retrieval-Augmented Generation} (RAG) faz uso direto desse conceito. Quando o usuário realiza uma consulta, o LLM invoca a ferramenta de busca semântica definida, passando como parâmetro a pergunta recebida. O banco de dados vetorial é então acionado e retorna um conjunto de trechos ranqueados conforme sua relevância. Com base nesse contexto recuperado, o LLM elabora uma resposta mais detalhada e precisa. Essa abordagem mostra-se especialmente útil em aplicações como atendimento ao cliente, treinamento automatizado em ambientes corporativos, suporte técnico e sistemas de perguntas e respostas sobre produtos ou processos internos \cite{langchain_rag_docs}.

Dessa forma, o \textit{Retrieval-Augmented Generation} (RAG) representa um avanço significativo na integração entre recuperação de informação e geração de linguagem natural, permitindo que modelos de IA acessem e utilizem conhecimento atualizado, verificável e contextualizado, sem necessidade de treinamento constante. Abaixo, apresenta-se um diagrama ilustrativo do funcionamento dessa técnica.

\begin{figure}[ht]
  %%% TODO: Referência da Imagem %%%
  \centering
  \includegraphics[width=\linewidth]{cap04-aplicacoes-e-usos/assets/rag-image-1.pdf}
  \caption{Arquitetura típica do Retrieval-Augmented Generation (RAG) utilizando Web Scraping para coleta de dados.}
  \label{fig:rag-image-1}
\end{figure}


\subsubsection{Extração de informações relevantes de páginas web}

A extração de informações é uma das partes mais importantes do processo de \textit{web scraping}, pois é nela que os dados coletados passam a ter valor prático. Normalmente, essa etapa é feita com regras fixas, como seletores CSS, expressões regulares e scripts que percorrem o HTML em busca de padrões conhecidos. O problema é que esse tipo de abordagem é bastante sensível a mudanças: qualquer alteração na estrutura da página pode quebrar o processo e exigir ajustes manuais. Isso faz com que projetos tradicionais de \textit{scraping} sejam difíceis de manter quando há muitas fontes ou quando o conteúdo muda com frequência.

Com o avanço da Inteligência Artificial (IA), esse cenário começou a mudar. Modelos de linguagem de grande porte (\textit{Large Language Models} — LLMs), como o GPT-4 e o Gemini 2.5, conseguem interpretar a estrutura e o contexto de uma página da web de maneira semelhante a um ser humano. Em vez de depender de seletores rígidos, é possível solicitar ao modelo que identifique, por exemplo, “todos os produtos e preços desta página” — e ele retorna apenas essas informações, mesmo que o HTML seja complexo ou mal formatado. Essa capacidade é fortalecida pelo uso dos chamados \textit{structured outputs}, que permitem definir de antemão o formato da resposta (por exemplo, em JSON ou XML), garantindo que os resultados sejam estruturados e prontos para uso \cite{openai_structured_outputs}.

Além disso, o uso de técnicas de \textit{function calling} e \textit{prompt engineering} permite conectar o modelo diretamente a funções específicas dentro do \textit{pipeline} de \textit{scraping}. Isso faz com que a IA atue como uma camada intermediária de compreensão: ela interpreta o conteúdo e decide o que é relevante antes de enviar os dados para o banco. Em muitos casos, essa combinação reduz drasticamente o tempo de manutenção e melhora a qualidade do resultado, principalmente quando há muitas variações de layout entre as páginas.

Outro recurso importante é a aplicação de modelos voltados para extração de entidades nomeadas (\textit{Named Entity Recognition} — NER). Eles são capazes de identificar automaticamente elementos como nomes, datas, valores e localidades dentro de um texto, o que ajuda a transformar dados brutos em informações realmente úteis. Esse tipo de técnica é bastante usado em cenários corporativos, por exemplo, para coletar informações de sites de concorrentes, portais de notícias, editais públicos ou bases científicas \cite{ibm_entidades_nomeadas}.

Em resumo, a IA trouxe uma nova camada de inteligência para o \textit{web scraping}. Em vez de apenas “raspar” conteúdo, os sistemas passam a entender o que estão coletando. Isso torna todo o processo mais flexível, escalável e conectado a aplicações mais complexas — como o \textit{Retrieval-Augmented Generation} (RAG), análise de tendências e treinamento automatizado de agentes de IA baseados em dados atualizados.