\section{Aplicações e Usos}

\subsection{Aplicações gerais e práticas}

Antes de entrar por áreas específicas, é válido trazer uma breve contextualização. Quando a raspagem de dados é combinada a modelos de IA, ela deixa de ser “coleta bruta” e passa a sustentar decisões do dia a dia — de acompanhar preços e notícias a apoiar diagnósticos e políticas públicas. Sempre que houver canais oficiais ou repositórios abertos (por exemplo, a coleção pública Common Crawl para textos da web, os catálogos acadêmicos Crossref e OpenAlex, ou o observatório de indicadores da Organização Mundial da Saúde), a coleta fica mais estável, reprodutível e auditável; quando esses canais não existem, a raspagem controlada ainda pode gerar valor, desde que venha acompanhada de limites de requisição, registro de versões e checagens de qualidade.


No varejo on-line, a coleta automatizada apoia o acompanhamento de preços, disponibilidade e tendências de consumo. Estudos econômicos que construíram índices a partir de milhões de preços coletados na web mostraram que esses índices andam de perto com os indicadores oficiais e ajudam a ler o mercado com mais rapidez. Revisões metodológicas em periódicos de estatística também discutem como transformar esses preços raspados em séries consistentes, comparando fórmulas e cuidados de qualidade.
No mercado financeiro, o uso de raspagem e IA aparece em dois eixos. Primeiro, na análise textual de documentos corporativos (como demonstrações e relatórios), que deu origem a dicionários e práticas de leitura automática hoje consolidadas em finanças. Segundo, no avanço dos “dados alternativos” (notícias, avaliações de produtos, sinais da web) para previsão, gestão de risco e monitoramento — tema que já conta com revisões amplas na literatura. Em paralelo, pesquisas recentes mostram que modelos modernos (incluindo redes em grafos) vêm ampliando o desempenho na detecção de fraude.


Em ciência de dados e pesquisa, repositórios abertos tornaram viável treinar e avaliar modelos em larga escala. A coleção Common Crawl, por exemplo, disponibiliza periodicamente textos coletados da web e é amplamente usada em pré-treinamento; para literatura científica, trabalhos sobre o OpenAlex e sobre o Crossref descrevem como esses catálogos estruturam obras, autores, citações e vínculos, o que permite bibliometria, busca semântica e recuperação de contexto com rigor. 
Na saúde, há duas frentes claras. Para suporte clínico com recuperação de evidências, surgiram levantamentos e protótipos que integram recuperação e geração (RAG) com bases biomédicas, discutindo benefícios e limites de precisão factual. Para vigilância epidemiológica, um marco foi o painel interativo da Universidade Johns Hopkins, descrito em revista médica de alto impacto, que integrou múltiplas fontes públicas para acompanhar a COVID-19 em tempo real — um modelo de organização de coleta, validação e abertura de dados em crises sanitárias. 


Em governo e políticas públicas, a literatura acadêmica e de organismos internacionais mostra como dados raspados ajudam a medir preços, acompanhar mercados de trabalho (ex.: vagas on-line) e produzir indicadores com maior frequência. Trabalhos em periódicos e working papers de confiança discutem ganhos e limitações: representatividade, vieses de cobertura e técnicas para transformar páginas em séries válidas para análise econômica.
Na mídia e comunicação, bases que agregam notícias em múltiplos idiomas são usadas para mapear eventos, lugares e o “tom” de coberturas ao longo do tempo, com documentação acadêmica do seu processo de codificação e atualização. Para entender consumo e confiança em notícias, o Digital News Report 2025 (Universidade de Oxford) oferece séries comparáveis entre países; e, no combate à desinformação, há levantamentos de referência que revisam métodos de checagem automatizada, conjuntos de dados e desafios de interpretabilidade.


Em síntese, em todos os domínios, o valor do web scraping aliado à IA depende de três cuidados práticos: priorizar fontes e estudos reconhecidos (artigos, relatórios técnicos de pesquisa), padronizar qualidade ao longo do ETL (registros de coleta, amostragem, validações) e delimitar riscos e escopo de uso (privacidade, direitos, atualização). Esses elementos transformam a coleta em evidência útil para produtos analíticos, modelos e decisões.

\subsection{Usos organizacionais por nível}

A aplicação do Web Scraping combinado ao uso de inteligência artificial pode ser compreendida de 
Forma mais estruturada quando analisada sob a ótica dos níveis organizacionais (operacional, tático e estratégico). Cada um desses níveis demanda informações com diferentes graus de detalhamento e propósito analítico que, quando integrados corretamente, são essenciais para transformar dados em inteligência organizacional.

No nível operacional, o objetivo está na automação de tarefas repetitivas e na eficiência de processos. O Web Scraping atua como ferramenta de coleta contínua e sistemática de dados que alimentam rotinas diárias, reduzindo a necessidade do trabalho manual. Entre as aplicações mais comuns estão o monitoramento de preços e estoques em e-commerces, a coleta de avaliações e comentários de clientes em plataformas e a geração automática de relatórios básicos de desempenho. Quando associado a modelos de IA, esses dados podem ser classificados e resumidos em um intervalo de tempo relativamente menor, permitindo respostas rápidas e atualizadas com base nas solicitações.

No nível tático, a raspagem de dados tem uma finalidade mais analítica. Os dados extraídos são normalmente integrados a sistemas de Business Intelligence (BI) e transformados em dashboards que mostram tendência de mercado, comportamentos de clientes e desempenho da empresa. Nessa ocasião, o uso de técnicas de aprendizado de máquina sobre os dados coletados permite identificar padrões e anomalias, enriquecendo relatórios gerenciais com insights preditivos.

No nível estratégico, a combinação de Web Scraping e Inteligência Artificial assume uma função de suporte à inovação e à formulação de políticas corporativas de longo prazo. As informações coletadas em larga escala podem alimentar modelos de previsão (forecasting), análises de competitividade e mecanismos de detecção de tendências emergentes. Empresas de tecnologia, finanças e varejo, por exemplo, utilizam pipelines de scraping e modelos de linguagem para antecipar mudanças no comportamento do consumidor, movimentações regulatórias ou novas oportunidades de mercado. Além disso, bases raspadas e tratadas adequadamente podem servir como insumo para projetos de inovação em produtos e serviços baseados em dados, fortalecendo a vantagem competitiva e a cultura data-driven das organizações. Nesse sentido,  o Web Scraping deixa de ser uma simples técnica de coleta e passa a representar um ativo estratégico, que sustenta decisões de alto impacto e orienta o posicionamento corporativo no mercado.

Em suma, os usos organizacionais do Web Scraping aliado à IA se estendem por todos os níveis da hierarquia empresarial, da automação operacional à inteligência estratégica, reforçando a importância dessa tecnologia como pilar da transformação digital e da competitividade baseada em dados.

\subsection{Aplicações práticas com modelos de linguagem de grande escala (LLM)}

Com o advento da inteligência artificial, principalmente dos modelos de linguagem de grande escala, observou-se uma oportunidade dentro do mercado para utilizar essa ferramenta para apoiar no atendimento externo e interno das companhias do mundo todo, que tradicionalmente eram feitos por funcionários que utilizavam bases de conhecimentos internas para responder dúvidas de clientes ou dentro da empresa. No entanto, pelo fato desses modelos terem um conhecimento limitado aos dados de treinamento, sua aplicação prática tornou-se desafiadora, uma vez que os dados privados das companhias não estariam disponíveis para que o modelo pudesse formular respostas adequadas. Por esse motivo, foram desenvolvidas algumas técnicas que serviram como base para que esses desafios fossem superados.

\subsubsection{Ajuste fino}

A primeira técnica utilizada para adaptar as respostas de um modelo de LLM, é o “ajuste fino” ou “fine-tuning”, que consiste no processo de adaptação de um modelo treinado previamente para tarefas ou casos de uso específicos. Sua ideia principal é de aproveitar o conhecimento existente do modelo como ponto de partida para aprender novas tarefas, o que permite uma otimização de custo por não precisar treinar um modelo do zero. 

O ajuste fino é considerado uma técnica que utiliza-se do aprendizado supervisionado, uma vez que para realizar o treinamento de um modelo de LLM para um domínio específico, é necessário fornecer um conjunto de dados rotulados. É nessa etapa que o Web Scraping pode atuar como uma ferramenta essencial para realizar a obtenção das informações, que eventualmente podem não estar disponíveis de forma oficial. Seu papel entra diretamente na capacidade de extrair dados rotulados a partir de páginas web que estejam alinhadas com o objetivo final do modelo que será treinado, transformando eles em dados rotulados que são utilizados dentro do processo de fine-tuning. A título de exemplo, podemos citar uma uma página web de perguntas e respostas de uma empresa, que contém dados rotulados (as respostas) e podem ser facilmente transformados para realizar o aprendizado supervisionado por meio do fine-tuning.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{cap04-aplicacoes-e-usos/assets/fine-tuning-image-1.pdf}
  \caption{Conjunto de dados do Hugging Face para fine-tuning de modelos de linguagem.}
  \label{fig:fine-tuning-image-1}
\end{figure}

%%% TODO: Referência da Imagem %%%

Essa técnica costuma ser oferecida como serviço na nuvem, diretamente nos servidores das empresas que possuem um LLM proprietário. Uma delas é a OpenAI, que permite realizar o envio de arquivos CSV contendo a variável independente e a dependente (alvo). Após a execução do processo de fine-tuning com o conjunto de dados enviado, gera-se um modelo com identificador único que pode ser utilizado posteriormente, seja por meio de chamadas de API, para integrar com aplicações empresariais, ou pela própria interface da OpenAI. Por outro lado, existem LLM que são disponibilizados de forma gratuitas através de plataformas como o Hugging Face e que podem ser baixados em servidores dedicados, sendo possível aplicar o mesmo processo de fine-tuning que é utilizado dentro da plataforma de empresas que possuem modelos proprietários. Abaixo, temos um diagrama de como esse cenário poderia ser implementado:

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{cap04-aplicacoes-e-usos/assets/fine-tuning-image-2.pdf}
  \caption{Arquitetura típica para fine-tuning de modelos de linguagem com dados obtidos por Web Scraping.}
  \label{fig:fine-tuning-image-2}
\end{figure}

%%% TODO: Referência da Imagem %%%

\subsubsection{RAG (Retrieval-Augmented Generation)}

O Retrieval-Augmented Generation (RAG) é um padrão de arquitetura no qual um modelo de linguagem (Large Language Model — LLM) gera respostas condicionadas a uma base de conhecimento externa, armazenada em um mecanismo que permite a busca semântica dos conteúdos. Esse acoplamento entre recuperação e geração reduz o risco de alucinações, mantém a atualidade do conteúdo sem necessidade de treinamentos frequentes e possibilita a citação explícita das fontes utilizadas. Em contraste com abordagens que internalizam o conhecimento no próprio modelo, o RAG desloca o conhecimento para fora do LLM, utilizando-o de forma controlada durante o processo de inferência.

Quando comparado ao fine-tuning, o RAG mostra-se mais adequado em contextos em que o conhecimento da organização é dinâmico, sujeito a alterações frequentes, ou quando há exigências de auditoria e restrições de acesso a dados sensíveis que não devem ser incorporados permanentemente ao modelo. O fine-tuning, por sua vez, mantém-se vantajoso em cenários nos quais se busca induzir estilo de escrita, formato de saída ou comportamentos específicos de tarefa, bem como generalizações a partir de exemplos rotulados. Na prática, ambas as abordagens são complementares: o RAG é mais apropriado para a gestão de conteúdo dinâmico e que possa conter citações, enquanto o fine-tuning é indicado para a adequação comportamental e de formato.

O pipeline típico do RAG inicia-se com a etapa de ingestão de conteúdo, na qual o Web Scraping desempenha papel fundamental na coleta automatizada de páginas autorizadas, como FAQs, políticas corporativas, manuais e documentações técnicas. O material coletado passa por processos de limpeza e normalização — por exemplo, conversão de HTML para texto puro ou Markdown —, sendo posteriormente enriquecido com metadados relevantes, como URL, título, data e versão. Essa etapa visa reduzir ruídos e garantir a consistência das informações que serão utilizadas.

Em seguida, realiza-se a segmentação textual, dividindo o conteúdo em passagens curtas com sobreposição controlada e alinhamento aos títulos das páginas. Essa técnica preserva a coesão textual e otimiza a etapa de recuperação semântica. Após a segmentação, cada trecho é convertido em uma representação vetorial (embedding), ou seja, uma forma numérica de representar o significado semântico do texto. Essa transformação é realizada por modelos especializados, como o text-embedding-3-large, da OpenAI. Os vetores resultantes são armazenados em bancos de dados vetoriais — como Chroma, Pinecone, Weaviate e pgvector —, que permitem a recuperação semântica das informações com base na similaridade entre a consulta e os vetores armazenados. Entre as principais métricas utilizadas destacam-se a Mean Reciprocal Rank (MRR) e a similaridade de cosseno.

A última etapa do pipeline envolve a integração com o modelo de linguagem, na qual se implementa a lógica de consulta semântica e recuperação das informações a partir do banco de dados vetorial. Atualmente, muitos LLMs são treinados com suporte a chamadas de ferramentas (tool calling), mecanismo que permite ao modelo, durante a inferência, acionar de forma programática uma função externa previamente definida. Essa função pode executar uma busca semântica e retornar seus resultados, os quais são adicionados ao histórico de mensagens da sessão, servindo como contexto adicional para o modelo gerar uma resposta ao usuário final.

Nesse contexto, o RAG faz uso direto desse conceito. Quando o usuário realiza uma consulta, o LLM invoca a ferramenta de busca semântica definida, passando como parâmetro a pergunta recebida. O banco de dados vetorial é então acionado e retorna um conjunto de trechos ranqueados conforme sua relevância. Com base nesse contexto recuperado, o LLM elabora uma resposta mais detalhada e precisa. Essa abordagem mostra-se especialmente útil em aplicações como atendimento ao cliente, treinamento automatizado em ambientes corporativos, suporte técnico e sistemas de perguntas e respostas sobre produtos ou processos internos.

Dessa forma, o RAG representa um avanço significativo na integração entre recuperação de informação e geração de linguagem natural, permitindo que modelos de IA acessem e utilizem conhecimento atualizado, verificável e contextualizado, sem necessidade de treinamento constante. Abaixo, apresenta-se um diagrama ilustrativo do funcionamento dessa técnica.

%%% Imagem RAG %%%

\subsubsection{Extração de informações relevantes de Páginas Web}

A extração de informações é uma das partes mais importantes do processo de Web Scraping, pois é nela que os dados coletados passam a ter valor prático. Normalmente, essa etapa é feita com regras fixas, como seletores CSS, expressões regulares e scripts que percorrem o HTML em busca de padrões conhecidos. O problema é que esse tipo de abordagem é bastante sensível a mudanças: qualquer alteração na estrutura da página pode quebrar o processo e exigir ajustes manuais. Isso faz com que projetos de scraping tradicionais sejam difíceis de manter quando há muitas fontes ou quando o conteúdo muda com frequência.


Com o avanço da Inteligência Artificial, esse cenário começou a mudar. Modelos de linguagem de grande porte (Large Language Models — LLMs), como o GPT-4 e o Gemini 1.5, conseguem interpretar a estrutura e o contexto de uma página da web de maneira semelhante a um ser humano. Em vez de depender de seletores rígidos, é possível pedir ao modelo que identifique, por exemplo, “todos os produtos e preços desta página” — e ele retorna apenas essas informações, mesmo que o HTML seja complexo ou mal formatado. Essa capacidade é fortalecida pelo uso dos chamados structured outputs, que permitem definir de antemão o formato da resposta (por exemplo, em JSON ou XML), garantindo que os resultados sejam estruturados e prontos para uso (OPENAI, 2024).


Além disso, o uso de técnicas de function calling e prompt engineering permite conectar o modelo diretamente a funções específicas dentro do pipeline de scraping. Isso faz com que a IA atue como uma camada intermediária de compreensão: ela interpreta o conteúdo e decide o que é relevante antes de enviar os dados para o banco. Em muitos casos, essa combinação reduz drasticamente o tempo de manutenção e melhora a qualidade do resultado, principalmente quando há muitas variações de layout entre as páginas.
Outro recurso importante é a aplicação de modelos voltados para extração de entidades nomeadas (Named Entity Recognition — NER). Eles são capazes de identificar automaticamente elementos como nomes, datas, valores e localidades dentro de um texto, o que ajuda a transformar dados brutos em informações realmente úteis. Esse tipo de técnica é bastante usado em cenários corporativos, por exemplo, para coletar informações de sites de concorrentes, portais de notícias, editais públicos ou bases científicas (IBM, O que é reconhecimento de entidades nomeadas).


Em resumo, a Inteligência Artificial trouxe uma nova camada de inteligência para o Web Scraping. Em vez de apenas “raspar” conteúdo, os sistemas passam a entender o que estão coletando. Isso torna todo o processo mais flexível, escalável e conectado a aplicações mais complexas — como o Retrieval-Augmented Generation (RAG), análise de tendências e treinamento automatizado de agentes de IA baseados em dados atualizados.