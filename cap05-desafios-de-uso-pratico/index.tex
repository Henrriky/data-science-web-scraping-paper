\section{Desafios de uso prático}

%% TODO: Fazer seção de escalabilidade
\subsection{Escalabilidade (custos computacionais)}
Escalar web scraping aliado à IA significa aumentar cobertura, frequência de atualização e profundidade de coleta sem degradar qualidade ou inviabilizar custos. O principal desafio é que, conforme o volume cresce, crescem também as requisições, o tamanho médio das páginas (especialmente quando exigem renderização de JavaScript) e a diversidade de layouts, o que eleva latência, taxas de erro e necessidade de infraestrutura distribuída para filas, tolerância a falhas e backoff. Além disso, mecanismos anti-bot, limites de cortesia (politeness) e restrições de robots/termos de uso impõem limites práticos que entram no cálculo de capacidade.

Do ponto de vista econômico e técnico, os pontos de pressão aparecem em: (a) coleta/recrawl — decidir quando revisitar para manter freshness sem explodir tráfego; (b) processamento/normalização — lidar com mudanças frequentes de HTML e evitar quebras em larga escala; (c) armazenamento e tráfego — volumes crescentes, versões e logs; (d) indexação e busca — em cenários com RAG, o custo de embeddings e de bases vetoriais cresce com a cardinalidade e com atualizações contínuas; (e) inferência — integrar IA na ponta adiciona custo por consulta e metas de latência. 

Em síntese, a escalabilidade aqui é um problema de trade-offs: freshness vs. custo, velocidade vs. qualidade, volume vs. conformidade (LGPD/robots/termos). Governança e observabilidade (medir cobertura, erro, latência e custo por documento) tornam-se parte do próprio desafio, pois sem elas não há como decidir onde e quanto escalar.

\subsection{Limitações técnicas}
A qualidade dos dados extraídos por Web Scraping é um fator crítico que pode comprometer todo o pipeline subsequente. Dados capturados com erros de parsing, seletores desatualizados, formatações inconsistentes ou incompletas afetam diretamente a confiabilidade das inferências ou gerações realizadas por modelos de IA. Além disso, conteúdos dinâmicos ou carregados via JavaScript (AJAX, lazy loading) complicam a extração automática, exigindo ferramentas que simulem navegação ou monitorem as requisições de rede. Scrapers que não lidam com essas variações acabam extraindo dados incorretos ou vazios — um risco especialmente danoso em sistemas que dependem de recuperação vetorial (RAG) para gerar respostas acuradas.

Do ponto de vista da robustez e operacionalização, o sistema de scraping enfrenta desafios práticos como bloqueios anti-bot (CAPTCHAs, detecção de fingerprinting, bloqueio por IP), mudanças frequentes na estrutura dos sites, latência, falhas de rede e manutenção constante dos scrapers. Também há trade-offs entre frescor dos dados e custo: realizar scraping “ao vivo” pode tornar o sistema lento ou suscetível a falhas, enquanto fazer atualizações periódicas pode deixar o índice “desatualizado”. Há ainda o risco de envenenamento de dados (inserção maliciosa) ou fragmentos irrelevantes, exigindo re-ranking ou filtros de validação. Em suma, construir um pipeline de Web Scraping integrado a IA requer não apenas técnicas sólidas de extração, mas uma arquitetura resiliente — capaz de detectar degradação, adaptar-se a variações e manter níveis aceitáveis de acurácia e disponibilidade ao longo do tempo.

\subsection{Aspectos legais e éticos}
O Web Scraping apresenta desafios legais significativos, envolvendo direitos autorais, termos contratuais de uso (termos de serviço) e regulamentações de proteção de dados pessoais. Mesmo que um site disponibilize conteúdo publicamente, a extração sistemática pode entrar em conflito com cláusulas que proíbem a raspagem ou com normas de privacidade, se houver coleta de informações identificáveis. Um exemplo emblemático é o caso hiQ Labs vs. LinkedIn: a hiQ raspava perfis públicos do LinkedIn para alimentar seus produtos de análise, mas o LinkedIn enviou notificação para barrar essa atividade, alegando violação de contratos e uso indevido de acesso. O Tribunal de Apelações da 9ª Região dos EUA inicialmente decidiu a favor da hiQ, entendendo que perfis públicos não são protegidos pela lei federal anti-hacking (CFAA) e que o bloqueio do LinkedIn poderia caracterizar prática anticompetitiva. Posteriormente, o caso voltou ao tribunal à luz de uma mudança interpretativa no âmbito da CFAA (decisão Van Buren) e acabou se encerrando em acordo, com hiQ reconhecendo que havia violado os termos de uso do LinkedIn e concordando em pagar indenização.

No plano ético, é fundamental agir com prudência e respeito a indivíduos e fontes. Recomenda-se anonimização ou agregação de dados, limitação de taxas de requisição (rate limiting), observância do protocolo robots.txt, transparência no propósito da coleta e salvaguarda dos direitos dos sites e dos usuários. Também é importante reconhecer que ética e legalidade se complementam — não basta estar legalmente “dentro da lei”, é necessário que o uso dos dados seja responsável, respeitoso e consciente, minimizando riscos de danos e respeitando os limites implícitos de privacidade e propriedade intelectual.