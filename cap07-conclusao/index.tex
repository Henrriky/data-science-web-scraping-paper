\section{Conclusão}

Este artigo analisou a integração entre as técnicas de web scraping e a Inteligência Artificial, demonstrando sua evolução de uma simples ferramenta de extração de dados para um componente estratégico nos ecossistemas de tecnologia. O objetivo principal foi explorar as aplicações, os desafios e as perspectivas futuras dessa sinergia, destacando seu papel fundamental desde os tradicionais pipelines de ETL até as modernas arquiteturas de Geração Aumentada por Recuperação (RAG) para LLMs.

Os resultados apresentados confirmam que a combinação de web scraping e IA transcende a automação da coleta de dados, transformando-se em um pilar para a inovação e a tomada de decisão em múltiplos níveis organizacionais — operacional, tático e estratégico. A capacidade de alimentar modelos de linguagem com informações atualizadas e contextuais, extraídas da web de forma automatizada, não apenas supera as limitações de conhecimento estático dos LLMs, mas também viabiliza aplicações de alto valor, como assistentes virtuais corporativos, análise de mercado em tempo real e sistemas de detecção de tendências.

No entanto, a implementação prática dessas tecnologias enfrenta limitações significativas. Os desafios técnicos, como a dificuldade de extrair conteúdo dinâmico e a necessidade de contornar mecanismos anti-bot, somam-se às barreiras legais e éticas, principalmente no que diz respeito à Lei Geral de Proteção de Dados (LGPD) e aos termos de serviço dos sites. A robustez e a escalabilidade dos pipelines de extração são, portanto, cruciais para garantir a qualidade e a conformidade dos dados que alimentam os modelos de IA.

Como sugestão para trabalhos futuros, recomenda-se a investigação de arquiteturas de scraping mais resilientes e adaptativas, que utilizem a própria IA para interpretar mudanças de layout em páginas web e otimizar a extração de informações. Adicionalmente, há um campo fértil para o desenvolvimento de frameworks de governança de dados que garantam a proveniência, a qualidade e a conformidade ética das informações coletadas para treinar e alimentar sistemas inteligentes, fortalecendo a confiança e a responsabilidade no uso de dados da web.