\section{Evolução do Tema e Estágio Comercial}
Com a consolidação da Web e de seus padrões básicos (HTTP, HTML e URLs), surgiram os primeiros agentes automáticos capazes de percorrer hiperlinks e coletar conteúdo em escala \cite{menczer_web_crawling}. O crescimento acelerado de páginas e a necessidade de indexação impulsionaram a distinção entre descoberta (crawling) e extração (scraping), ainda que, nessa fase inicial, muitas soluções fossem rudimentares e voltadas a tarefas de arquivamento e busca \cite{ferrara2014}. Essa base técnica foi viabilizada pelo próprio desenho aberto da Web e por sua padronização progressiva, coordenada por organismos como o W3C \cite{w3c_webarch}.

À medida que sites adotaram layouts mais ricos e conteúdo gerado dinamicamente, a extração migrou de scripts ad hoc para pipelines com etapas explícitas (coleta, parsing, normalização, validação). A literatura acadêmica começou a organizar o campo com classificações de técnicas (por exemplo, extração baseada em DOM, padrões e aprendizado), além de separar aplicações “corporativas” e “sociais” \cite{ferrara2014,lotfi2022}. O foco passou a ser qualidade, robustez e reuso de componentes — um passo essencial para que a prática deixasse de ser artesanal e se tornasse parte do ecossistema de data engineering.

Com o avanço de aplicações ricas (AJAX, lazy loading), a extração precisou lidar com renderização de cliente, sessões e políticas anti-automação. Em paralelo, amadureceram diretrizes de acesso responsável (identificação de user-agent, rate limiting, back-off, respeito a políticas de acesso) e o próprio protocolo de exclusão de robôs, robots.txt, passou de convenção de fato a padrão formal: a RFC 9309 especifica linguagem, cache e tratamento de erros, reduzindo ambiguidades operacionais entre quem coleta e quem publica \cite{rfc9309_robots}. Essa etapa consolidou um equilíbrio prático entre viabilidade técnica e governança do tráfego automatizado.

Com a maturidade dos pipelines, o mercado evoluiu de “raspar páginas” para entregar dados e conhecimento como serviços — índices pesquisáveis, catálogos temáticos e camadas de enriquecimento que abstraem a complexidade da coleta \cite{groupbwt_saas,grepsr_daas}. Organizações passaram a decidir entre operar seus próprios pipelines ou consumir dados/infraestruturas de terceiros, avaliando custo de manutenção, cobertura, atualização (freshness) e requisitos legais \cite{mordor_webscraping_market_2025}. Na prática, a atenção deslocou-se do “como raspar” para SLA, governança e auditabilidade do que é entregue. (De forma geral, diretrizes de acesso e o padrão robots.txt seguem como referências para conformidade técnica).

A popularização de modelos de linguagem (LLMs) evidenciou um limite: o conhecimento interno ao modelo é paramétrico (aprendido no treino) e, portanto, envelhece e não carrega, por si, proveniência. A resposta arquitetural foi integrar recuperação externa ao processo de geração, a Retrieval-Augmented Generation (RAG), que combina memória paramétrica do LLM com memória não-paramétrica consultável em tempo de execução \cite{lewis2020}. Na prática, isso requer conteúdo bem coletado e estruturado (muitas vezes por pipelines de scraping), depois indexado/embarcado para recuperação semântica e citação de fontes. O resultado é um estágio abertamente comercial: provedores em nuvem documentam padrões e motores gerenciados de RAG, evidenciando que a coleta deixou de ser fim em si mesma para se tornar capacidade de atualização, checabilidade e contexto para aplicações de IA \cite{aws_rag2025,langchain_rag_docs}.
