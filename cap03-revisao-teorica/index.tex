\section{Revisão Teórica}

\subsection{Conceitos-base: Web Scraping X Web Crawling}

Web Scraping é o processo que atua de forma assertiva, automatizando a extração de conjuntos de dados específicos de páginas na internet, utilizando ferramentas que permitem organizar essas informações em um formato estruturado, o que facilita sua transformação e utilização em linguagens de programação. Por outro lado, o Web Crawling é o processo ou algoritmo que, de forma automática, navega entre diferentes páginas interligadas na internet através de uma cadeia de hyperlinks, tendo como principal objetivo coletar o conteúdo delas para realizar diferentes tarefas, como a indexação de sites por mecanismos de pesquisa ou o treinamento de modelos de IA, sendo amplamente utilizado por grandes empresas como Google, Bing e OpenAI. Na prática, muitos pipelines utilizam um crawler para mapear páginas e um scraper para extrair os dados de interesse (FERRARA et al., 2012; 2014).

\subsection{Web Scraping no pipeline de ETL}

Web Scraping é o processo que atua de forma assertiva, automatizando a extração de conjuntos de dados específicos de páginas na internet, utilizando ferramentas que permitem organizar essas informações em um formato estruturado, o que facilita sua transformação e utilização em linguagens de programação. Por outro lado, o Web Crawling é o processo ou algoritmo que, de forma automática, navega entre diferentes páginas interligadas na internet através de uma cadeia de hyperlinks, tendo como principal objetivo coletar o conteúdo delas para realizar diferentes tarefas, como a indexação de sites por mecanismos de pesquisa ou o treinamento de modelos de IA, sendo amplamente utilizado por grandes empresas como Google, Bing e OpenAI. Na prática, muitos pipelines utilizam um crawler para mapear páginas e um scraper para extrair os dados de interesse (FERRARA et al., 2012; 2014).

\subsection{IA e LLM: Dos dados brutos ao contexto e treinamento}

No contexto de ETL (Extract, Transform, Load), o scraping atua principalmente na extração, ou seja, inicialmente são obtidos dados de páginas HTML, API’s públicas, arquivos estáticos (CSV, JSON) e documentos (PDFs), que posteriormente passam por transformações (limpeza, normalização e enriquecimento) e são carregados em data lakes, data warehouse ou banco de dados SQL e NoSQL. A literatura da modelagem e integração dos dados destaca o ETL como um dos pilares para as análises em Ciência de Dados (KIMBALL; ROSS, 2013), o que reforça a importância do scraping para essa área.

\subsection{RAG e bases vetoriais}

A combinação de scraping com Inteligência Artificial, amplia significativamente as possibilidades de uso dos dados. Quando inseridos em um pipeline de Extract, Transform, Load (ETL) adequado, os dados extraídos podem ser organizados e utilizados em treinamentos supervisionados e não supervisionados de modelos de IA. No caso específico de linguagens de larga escala (LLMs), o conhecimento armazenado é paramétrico, ou seja, limitado aos dados utilizados em sua fase de treinamento. Essa característica dificulta a obtenção de fatos e fontes recentes. Para superar tais restrições, arquiteturas que integram memória externa consultável (RAG), nas quais o Web Scraping desempenha papel essencial no fornecimento das informações que alimentam esse processo (LEWIS et al., 2020).

\subsection{Qualidade, robustez e desafios práticos}

A técnica Retrieval-Augmented Generation (RAG), que combina memória paramétrica e não paramétrica, foi formalizada por Lewis et al. (2020), como uma forma de melhorar as respostas de modelos de linguagem de larga escala (LLMs), permitindo que eles consultem uma base de conhecimento externa além do que foi aprendido no treinamento original, funcionanando como um complemento de contexto para a resposta final. Para isso, os conteúdos coletados, muitas vezes por meio de Web Scraping, passam pela etapa de criação de embeddings, que são representações numéricas de textos, imagens ou áudios, armazenadas em bancos de dados vetoriais. Durante a inferência, o sistema recupera os itens mais relevantes em relação à pergunta do usuário e os adiciona ao contexto da LLM, o que aumenta a precisão, possibilita acesso a informações mais recentes e melhora a capacidade de citar fontes. As implementações mais comuns utilizam métricas matemáticas, como a similaridade de cosseno ou o algoritmo de Maximal Marginal Relevance (MMR), para selecionar os conteúdos mais adequados, embora recentes estudos alertam para limitações para a métrica de similaridade de cosseno quando aplicada em espaços vetoriais complexos (Steck et al., 2024).”

\subsection{Aspectos legais e éticos}

A qualidade dos dados extraídos por Web Scraping é um fator crítico que pode comprometer todo o pipeline subsequente. Dados capturados com erros de parsing, seletores desatualizados, formatações inconsistentes ou incompletas afetam diretamente a confiabilidade das inferências ou gerações realizadas por modelos de IA. Além disso, conteúdos dinâmicos ou carregados via JavaScript (AJAX, lazy loading) complicam a extração automática, exigindo ferramentas que simulem navegação ou monitorem as requisições de rede. Scrapers que não lidam com essas variações acabam extraindo dados incorretos ou vazios — um risco especialmente danoso em sistemas que dependem de recuperação vetorial (RAG) para gerar respostas acuradas.

Do ponto de vista da robustez e operacionalização, o sistema de scraping enfrenta desafios práticos como bloqueios anti-bot (CAPTCHAs, detecção de fingerprinting, bloqueio por IP), mudanças frequentes na estrutura dos sites, latência, falhas de rede e manutenção constante dos scrapers. Também há trade-offs entre frescor dos dados e custo: realizar scraping “ao vivo” pode tornar o sistema lento ou suscetível a falhas, enquanto fazer atualizações periódicas pode deixar o índice “desatualizado”. Há ainda o risco de envenenamento de dados (inserção maliciosa) ou fragmentos irrelevantes, exigindo re-ranking ou filtros de validação. Em suma, construir um pipeline de Web Scraping integrado a IA requer não apenas técnicas sólidas de extração, mas uma arquitetura resiliente — capaz de detectar degradação, adaptar-se a variações e manter níveis aceitáveis de acurácia e disponibilidade ao longo do tempo.