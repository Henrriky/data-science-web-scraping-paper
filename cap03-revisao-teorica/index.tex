\section{Revisão Teórica}

\subsection{Conceitos-base: \textit{Web Scraping} x \textit{Web Crawling}}

\textit{Web scraping} é o processo que automatiza a extração de conjuntos de dados específicos de páginas na internet, utilizando ferramentas que organizam essas informações em formato estruturado, o que facilita sua transformação e utilização por meio de linguagens de programação. Por outro lado, o \textit{web crawling} é o processo ou algoritmo que, de forma automática, navega entre diferentes páginas interligadas na internet por meio de uma cadeia de \textit{hyperlinks}, tendo como principal objetivo coletar o conteúdo dessas páginas para realizar tarefas como indexação de sites por mecanismos de busca ou treinamento de modelos de IA. Esse método é amplamente utilizado por grandes empresas como Google, Bing e OpenAI. Na prática, muitos \textit{pipelines} utilizam um \textit{crawler} para mapear páginas e um \textit{scraper} para extrair os dados de interesse \cite{ferrara2014}.

\subsection{\textit{Web Scraping} no pipeline de ETL}

No contexto de ETL (\textit{Extract, Transform, Load}), o \textit{web scraping} atua principalmente na fase de extração, ou seja, inicialmente são obtidos dados de páginas HTML, APIs públicas, arquivos estáticos (CSV, JSON) e documentos (PDFs). Posteriormente, esses dados passam por transformações — como limpeza, normalização e enriquecimento — e são carregados em \textit{data lakes}, \textit{data warehouses} ou bancos de dados SQL e NoSQL. A literatura de modelagem e integração de dados destaca o ETL como um dos pilares para análises em ciência de dados \cite{kimball2013}, o que reforça a importância do \textit{web scraping} para essa área.

\subsection{IA e LLMs: dos dados brutos ao contexto e treinamento}

A combinação de \textit{web scraping} com Inteligência Artificial (IA) amplia significativamente as possibilidades de uso de dados. Quando inseridos em um \textit{pipeline} de ETL adequado, os dados extraídos podem ser organizados e utilizados em treinamentos supervisionados e não supervisionados de modelos de IA. No caso específico de modelos de linguagem de larga escala (LLMs), o conhecimento armazenado é paramétrico, ou seja, limitado aos dados utilizados em sua fase de treinamento. Essa característica dificulta a obtenção de fatos e fontes recentes. Para superar tais restrições, surgiram arquiteturas que integram memória externa consultável, conhecidas como \textit{retrieval-augmented generation} (RAG), nas quais o \textit{web scraping} desempenha papel essencial no fornecimento das informações que alimentam esse processo \cite{lewis2020}.

\subsection{RAG e bases vetoriais}

O \textit{Retrieval-Augmented Generation} (RAG) é uma técnica que combina memória paramétrica e não paramétrica, permitindo que modelos de linguagem de larga escala (LLMs) consultem uma base de conhecimento externa além do que foi aprendido durante o treinamento original. Essa abordagem foi formalizada por Lewis et al. \cite{lewis2020} como uma forma de melhorar a precisão e a contextualização das respostas geradas por LLMs.

Dessa maneira, o RAG representa uma evolução dos modelos puramente paramétricos, ao incorporar mecanismos de recuperação de informação que possibilitam integrar conhecimento dinâmico e verificável ao processo de geração. Esse tipo de arquitetura tem sido amplamente adotado em aplicações de IA que demandam atualidade e transparência das fontes, como assistentes corporativos, sistemas de suporte e mecanismos de busca semântica \cite{aws_rag2025, langchain_rag_docs}.